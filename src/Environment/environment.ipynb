{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 11:54:07.656945: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 11:54:07.710237: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-29 11:54:07.710275: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-29 11:54:07.711677: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-29 11:54:07.719086: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-29 11:54:07.719828: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-29 11:54:08.709743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Optional, Text, List, Dict, Tuple, Union\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tf_agents.agents import tf_agent\n",
    "from tf_agents.drivers import driver\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.policies import tf_policy\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.typing import types\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "from Components.feature_extractor import FeatureExtractor\n",
    "from Components.data_processing import data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
    "  \"\"\"Base class for Bandit Python environments.\n",
    "\n",
    "  Every bandit Python environment should derive from this class.\n",
    "  Subclasses need to implement functions _observe() and _apply_action().\n",
    "\n",
    "  Usage:\n",
    "\n",
    "  To receive the first observation, the environment's reset() function should be\n",
    "  called. To take an action, use the step(action) function. The time step\n",
    "  returned by step(action) will contain the reward and the next observation.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      observation_spec: types.NestedArray,\n",
    "      action_spec: types.NestedArray,\n",
    "      reward_spec: Optional[types.NestedArray] = None,\n",
    "      name: Optional[Text] = None,\n",
    "  ):\n",
    "    self._observation_spec = observation_spec\n",
    "    self._action_spec = action_spec\n",
    "    self._reward_spec = reward_spec\n",
    "    self._name = name\n",
    "    super(BanditPyEnvironment, self).__init__()\n",
    "\n",
    "  def _reset(self) -> ts.TimeStep:\n",
    "    \"\"\"Returns a time step containing an observation.\n",
    "\n",
    "    It should not be overridden by Bandit environment implementations.\n",
    "\n",
    "    Returns:\n",
    "      A time step of type FIRST containing an observation.\n",
    "    \"\"\"\n",
    "    return ts.restart(\n",
    "        self._observe(),\n",
    "        batch_size=self.batch_size,\n",
    "        reward_spec=self.reward_spec(),\n",
    "    )\n",
    "\n",
    "  def _step(self, action: types.NestedArray) -> ts.TimeStep:\n",
    "    \"\"\"Returns a time step containing the reward for the action taken.\n",
    "\n",
    "    The returning time step also contains the next observation.\n",
    "    It should not be overridden by bandit environment implementations.\n",
    "\n",
    "    Args:\n",
    "      action: The action taken by the Bandit policy.\n",
    "\n",
    "    Returns:\n",
    "      A time step of type LAST containing the reward for the action taken and\n",
    "      the next observation.\n",
    "    \"\"\"\n",
    "    # This step will take an action and return a reward.\n",
    "    reward = self._apply_action(action)\n",
    "    return ts.termination(self._observe(), reward)\n",
    "\n",
    "  def action_spec(self) -> types.NestedArraySpec:\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self) -> types.NestedArraySpec:\n",
    "    return self._observation_spec\n",
    "\n",
    "  def reward_spec(self) -> types.NestedArraySpec:\n",
    "    return self._reward_spec\n",
    "\n",
    "  def _empty_observation(self):\n",
    "    return tf.nest.map_structure(\n",
    "        lambda x: np.zeros(x.shape, x.dtype), self.observation_spec()\n",
    "    )\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _apply_action(self, action: types.NestedArray) -> types.Float:\n",
    "    \"\"\"Applies `action` to the Environment and returns the corresponding reward.\n",
    "\n",
    "    Args:\n",
    "      action: A value conforming action_spec that will be taken as action in the\n",
    "        environment.\n",
    "\n",
    "    Returns:\n",
    "      A float value that is the reward received by the environment.\n",
    "    \"\"\"\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def _observe(self) -> types.NestedArray:\n",
    "    \"\"\"Returns an observation.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def name(self) -> Optional[Text]:\n",
    "    return self._name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandit Environment for Model Selction in Anomaly Detection\n",
    "\n",
    "To define an environment for MAB Environment, we first need to define our observation and action space.\n",
    "\n",
    "**Observation Space** : The Observation Space is composed of all the selected features from the time series. Each Subsequence of the time series will be passed to the feature extractor component. This will return the necessary features to the observation space.\n",
    "\n",
    "**Action Space**: The Action Space consists of the models that will be selected based on the given observation by the agent.\n",
    "\n",
    "For the agent to have all the necessary information to make a good policy, the following functions need to be implemented.\n",
    "\n",
    "* **_observe**: This function will call the feature_extractor module and return all the relevant features.\n",
    "\n",
    "* **_reset**: This function initializes our environment. All the parameters will be reset back to their initial value.\n",
    "\n",
    "* **_apply_action**: Applies the 'action' given by the agent and returns the corresponding reward.\n",
    "\n",
    "* **_step**: This will return a time step in the environment. This includes the reward from the current action as well as the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialEnv(BanditPyEnvironment):\n",
    "\n",
    "    def __init__(self, list_pred_sc: List[Union[int, float]], list_thresholds: List[float], list_gtruth: List[float], model_path = '../../saved_models/'):\n",
    "\n",
    "        # Length of the testing data, number of models\n",
    "        self.len_data = len(list_pred_sc[0])\n",
    "        self.num_models = len(list_pred_sc)\n",
    "\n",
    "        #List of ground truth labels\n",
    "        self.gtruth = list_gtruth\n",
    "        \n",
    "        # Get the list of pretrained models\n",
    "        self.model_path = model_path \n",
    "        self.model_list = [f for f in os.listdir(self.model_path) if f.endswith('.sav')]\n",
    "\n",
    "        # Extract the model names\n",
    "        self.model_names = [f.split('_')[0] for f in self.model_list]\n",
    "        \n",
    "        # Raw scores and thresholds of the testing data\n",
    "        self.list_pred_sc = list_pred_sc\n",
    "        self.list_thresholds = list_thresholds\n",
    "\n",
    "        # Scale the raw scores/thresholds and save each scaler\n",
    "        self.scaler = []\n",
    "        self.list_scaled_sc = []\n",
    "        self.list_scaled_thresholds = []\n",
    "        for i in range(self.num_models):\n",
    "            scaler_tmp = MinMaxScaler()\n",
    "            self.list_scaled_sc.append(scaler_tmp.fit_transform(self.list_pred_sc[i].reshape(-1,1)))\n",
    "            self.scaler.append(scaler_tmp)\n",
    "            self.list_scaled_thresholds.append(scaler_tmp.transform(self.list_thresholds[i].reshape(-1,1)))\n",
    "\n",
    "        # Extract predictions\n",
    "        self.list_pred = []\n",
    "        for i in range(self.num_models):\n",
    "            pred_tmp = np.zeros(self.len_data)\n",
    "            for length in range(self.len_data):\n",
    "                if self.list_scaled_sc[i][length] > self.list_scaled_thresholds[i]:\n",
    "                    pred_tmp[length] = 1\n",
    "            self.list_pred.append(pred_tmp)\n",
    "\n",
    "        # Extract prediction-concensus confidence\n",
    "        self.list_concensus_conf = [] # how many models have predicted 1 (anomaly)\n",
    "        for length in range(self.len_data):\n",
    "            num_a_tmp=0 # number of models have predicted 1 (anomaly)\n",
    "            for i in range(self.num_models):\n",
    "                if self.list_scaled_sc[i][length] > self.list_scaled_thresholds[i]:\n",
    "                    num_a_tmp += 1\n",
    "            self.list_concensus_conf.append(num_a_tmp/self.num_models)\n",
    "\n",
    "        # Extract distance-to-threshold confidence\n",
    "        self.dist_conf=[]\n",
    "        for length in range(self.len_data):\n",
    "            dist_tmp = []\n",
    "            for i in range(self.num_models):\n",
    "                dist_tmp.append(self.list_scaled_sc[i][length] - self.list_scaled_thresholds[i])\n",
    "            self.dist_conf.append(dist_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelSelectionEnv(InitialEnv):\n",
    "\n",
    "    def __init__(self, list_pred_sc, list_thresholds, list_gtruth, time_series_dir):\n",
    "\n",
    "        super().__int__(list_pred_sc, list_thresholds, list_gtruth)\n",
    "\n",
    "        self.time_series = pd.read_csv(time_series_dir)\n",
    "        self.subsequences = data_process(time_series_dir)\n",
    "\n",
    "        action_spec = array_spec.BoundedArraySpec(shape=(2,), dtype=np.int32, minimum=0, maximum = 1, name='Select Models')\n",
    "        observation_spec = array_spec.BoundedArraySpec(shape=(30,), dtype=np.int32, name='observation')\n",
    "\n",
    "    def _reset(self)\n",
    "\n",
    "    def _observe(self):\n",
    "\n",
    "        dodgers_features = FeatureExtractor()\n",
    "        self._observation = dodgers_features.feature_extractor('Dodgers')\n",
    "        return self._observation\n",
    "    \n",
    "    def _apply_action(self, action):\n",
    "\n",
    "        if action == 0:\n",
    "            model = pickle.load(open(f'../../saved_models/iforest_dodgers_v1.sav','rb'))\n",
    "            feats = self._observe()\n",
    "            score = model.decision_function(feats)\n",
    "\n",
    "        elif action == 1:\n",
    "            model = pickle.load(open(f'../../saved_models/osvm_dodgers_v1.sav', 'rb'))\n",
    "            feats = self._observe()\n",
    "            score = model.decision_function(feats)\n",
    "\n",
    "            reward = self._reward_function(score)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _reward_function(self, score):\n",
    "\n",
    "        if self.gtruth[self.pointer]==1: # If the ground truth is 1 anomaly\n",
    "            if observation[2]==1: # If the model predicts 1 anomaly correctly - True Positive (TP)\n",
    "                reward = 1\n",
    "            else: # If the model predicts 0 normal incorrectly - False Negative (FN)\n",
    "                reward = -1.5\n",
    "        else: # If the ground truth is 0 normal\n",
    "            if observation[2]==1: # If the model predicts 1 anomaly incorrectly - False Positive (FP)\n",
    "                reward = -0.5\n",
    "            else: # If the model predicts 0 normal correctly - True Negative (TN)\n",
    "                reward = 0.1\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
