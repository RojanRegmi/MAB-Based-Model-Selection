{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-16 12:15:42.445251: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-16 12:15:42.491794: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-16 12:15:42.491836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-16 12:15:42.493081: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-16 12:15:42.500339: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-16 12:15:42.501226: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-16 12:15:43.655077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.Environment.environment import MyModelSelectionEnv\n",
    "from src.utils import train_test_anomaly\n",
    "from src.Environment import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_agents.bandits.agents as bandit_agents\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import StepType, restart\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents import data_converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data and Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../datasets/Dodgers/101-freeway-traffic.test.out'\n",
    "\n",
    "columns = ['value', 'anomaly']\n",
    "\n",
    "df = pd.read_csv(file_path, names=columns, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_data = train_test_anomaly(df)\n",
    "\n",
    "list_threshold = [-0.03, +5]\n",
    "list_gtruth = test_data['anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_np = test_data['value'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyModelSelectionEnv(test_data, list_thresholds=list_threshold, list_gtruth=list_gtruth)\n",
    "environment = tf_py_environment.TFPyEnvironment(env) # Converts the PyEnvironment to TFEnvironment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Neural Epsilon Greedy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = environment.action_spec()\n",
    "observation_spec = environment.time_step_spec().observation\n",
    "\n",
    "\n",
    "EPSILON = 0.1\n",
    "LAYERS = (50, 50, 50)\n",
    "LR = 0.005\n",
    "\n",
    "TRAINING_LOOPS = 500\n",
    "STEPS_PER_LOOP = 1\n",
    "\n",
    "network = q_network.QNetwork(\n",
    "          input_tensor_spec=observation_spec,\n",
    "          action_spec=action_spec,\n",
    "          fc_layer_params=LAYERS\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': TensorSpec(shape=(159,), dtype=tf.float64, name='observation')})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps_agent = bandit_agents.neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(action_spec=action_spec, time_step_spec=environment.time_step_spec(), reward_network=network ,optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        epsilon=EPSILON,\n",
    "        emit_policy_info='predicted_rewards_mean',\n",
    "        info_fields_to_inherit_from_greedy=['predicted_rewards_mean'])\n",
    "\n",
    "eps_agent.initialize()\n",
    "\n",
    "TRAINING_LOOPS = 500\n",
    "STEPS_PER_LOOP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "      env=environment,\n",
    "      policy=eps_agent.collect_policy,\n",
    "      num_steps=STEPS_PER_LOOP * environment.batch_size\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function where.<locals>.per_field_where at 0x7419656cab80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='60'\n",
       "                  max='60',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  60\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TimeStep(\n",
       " {'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
       "  'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.17], dtype=float32)>,\n",
       "  'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       "  'observation': <tf.Tensor: shape=(1, 159), dtype=float64, numpy=\n",
       " array([[ 2.92070000e+04,  9.16500000e+00,  2.92070000e+04,\n",
       "          5.96061224e+04,  2.37800000e-01,  1.00000000e+01,\n",
       "          4.00000000e+01, -1.00000000e+00,  3.20000000e+01,\n",
       "          2.00000000e-02,  4.00000000e-02,  6.00000000e-02,\n",
       "          8.00000000e-02,  1.00000000e-01,  1.20000000e-01,\n",
       "          1.40000000e-01,  1.60000000e-01,  1.80000000e-01,\n",
       "          2.00000000e-01,  7.04900000e-01,  1.78300000e-01,\n",
       "          2.55940000e+00,  1.45690000e+00,  4.97220000e+00,\n",
       "          1.88210000e+00,  5.43070000e+00,  2.27450000e+00,\n",
       "          2.16000000e-01,  1.93740000e+00,  3.50660000e+00,\n",
       "          1.43680000e+00,  1.47192000e+01,  1.00809000e+01,\n",
       "          1.03752000e+01,  4.36290000e+00,  5.51380000e+00,\n",
       "          4.86900000e-01,  6.92800000e-01,  1.19460000e+00,\n",
       "          7.23500000e-01,  1.42416000e+01,  3.14950000e+00,\n",
       "          4.60960000e+00,  5.50350000e+00,  6.91010000e+00,\n",
       "          1.05950000e+01,  4.00000000e+00,  1.20000000e+01,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "          7.50700000e-01,  2.90000000e+01, -1.40600000e+00,\n",
       "          1.97700000e-01,  6.96400000e-01,  4.15600000e-01,\n",
       "          6.96400000e-01,  4.15600000e-01,  1.94000000e-01,\n",
       "          2.87500000e-01,  5.67500000e-01,  2.41000000e-02,\n",
       "          5.67500000e-01,  2.87500000e-01,  1.94000000e-01,\n",
       "         -3.65355000e+01, -2.81069000e+01,  8.38124000e+01,\n",
       "          1.13192900e+02, -1.60434000e+01, -5.46884000e+01,\n",
       "          2.21216500e+02, -4.58203000e+01,  1.80600000e+01,\n",
       "          8.69904000e+01,  3.34901000e+01,  5.79805000e+01,\n",
       "          4.20000000e+01,  7.93000000e-02,  4.40000000e+01,\n",
       "          1.87400000e+01,  1.34408000e+01,  1.78571000e+01,\n",
       "          2.24500000e-01,  2.10000000e+01,  1.55000000e+01,\n",
       "          2.10000000e+01,  0.00000000e+00,  1.40000000e+01,\n",
       "         -1.00000000e+00,  1.40000000e+01,  1.00000000e+00,\n",
       "          4.30000000e+01,  1.70000000e+01,  4.00000000e+01,\n",
       "          2.41690000e+01,  8.80570700e+02, -2.70000000e-02,\n",
       "         -1.29000000e-02,  1.77299000e+01, -1.31780000e+00,\n",
       "         -1.39226002e+04,  8.72700000e-01,  1.74360000e+00,\n",
       "          8.00000000e+00,  4.40000000e+01,  0.00000000e+00,\n",
       "          4.09800000e-01, -1.20000000e-03,  1.62339000e+01,\n",
       "          5.08300000e-01,  1.52628000e+01,  8.75000000e+02,\n",
       "          2.32952400e+02,  5.65400000e-01,  2.10190000e+00,\n",
       "          3.99000000e+00,  5.95790000e+00,  7.65040000e+00,\n",
       "          9.30050000e+00,  1.13560000e+01,  1.42862000e+01,\n",
       "          1.83544000e+01,  1.51006000e+01,  1.65050000e+01,\n",
       "          2.00378000e+01,  2.24721000e+01,  2.18874000e+01,\n",
       "          2.04623000e+01,  1.93225000e+01,  1.93590000e+01,\n",
       "          2.14696000e+01,  2.19060000e+00,  1.50900000e+01,\n",
       "          1.63706000e+01,  1.96365000e+01,  2.16680000e+01,\n",
       "          2.05068000e+01,  1.82265000e+01,  1.56333000e+01,\n",
       "          1.30642000e+01,  1.11382000e+01,  2.27707200e+02,\n",
       "          2.67998000e+02,  3.85593400e+02,  4.69500900e+02,\n",
       "          4.20530600e+02,  3.32204400e+02,  2.44400100e+02,\n",
       "          1.70673400e+02,  1.24060100e+02,  1.80000000e+01]])>}),\n",
       " ())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = f'../agent_outputs'\n",
    "\n",
    "\n",
    "\n",
    "trainer.train(\n",
    "    root_dir=output_dir,\n",
    "    agent=eps_agent,\n",
    "    environment=environment,\n",
    "    training_loops=TRAINING_LOOPS,\n",
    "    steps_per_loop=STEPS_PER_LOOP,\n",
    "    save_policy=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up LinUCB Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_ALPHA = 10.0\n",
    "\n",
    "linucb_agent = bandit_agents.lin_ucb_agent.LinearUCBAgent(\n",
    "        time_step_spec=environment.time_step_spec(),\n",
    "        action_spec=environment.action_spec(),\n",
    "        tikhonov_weight=0.001,\n",
    "        alpha=AGENT_ALPHA,\n",
    "        dtype=tf.float32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'../agent_outputs/lin_UCB'\n",
    "\n",
    "TRAINING_LOOPS = 500\n",
    "STEPS_PER_LOOP = 2\n",
    "\n",
    "trainer.train(\n",
    "    root_dir=output_dir,\n",
    "    agent=linucb_agent,\n",
    "    environment=environment,\n",
    "    training_loops=TRAINING_LOOPS,\n",
    "    steps_per_loop=STEPS_PER_LOOP,\n",
    "    save_policy=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'../agent_outputs/policy_194/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_policy = tf.saved_model.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_state = saved_policy.get_initial_state(batch_size=1)\n",
    "policy_state\n",
    "\n",
    "time_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_policy.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_tf = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_np = env._observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_type = tf.constant(0, dtype=tf.int32)\n",
    "reward = tf.constant(0.0, dtype=tf.float32)\n",
    "discount = tf.constant(1.0, dtype=tf.float32)\n",
    "observation = tf.expand_dims(tf.convert_to_tensor(feat_np, dtype=tf.float64), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = ts.TimeStep(step_type, reward, discount, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m step_type_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m      2\u001b[0m reward_1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m      3\u001b[0m discount_1 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;241m0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32, shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "step_type_1 = tf.constant(0, dtype=tf.int32, shape=(1,))\n",
    "reward_1 = tf.constant(0, dtype=tf.float32, shape=(1,))\n",
    "discount_1 = tf.constant(0, dtype=tf.float32, shape=(1,))\n",
    "observation_1 = tf.random.uniform((159,), minval=0.0, maxval=1.0, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_1 = (step_type_1, reward_1, discount_1,observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_step = saved_policy.action(time_step_1, policy_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
