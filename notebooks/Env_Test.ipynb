{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-10 22:47:17.344502: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-10 22:47:17.344541: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-10 22:47:17.346175: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-10 22:47:17.354823: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-10 22:47:18.546040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Environment.environment import MyModelSelectionEnv\n",
    "from src.utils import train_test_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_agents.bandits.agents as bandit_agents\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.networks import q_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../datasets/Dodgers/101-freeway-traffic.test.out'\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = {'-1': 'value', '1': 'anomaly'}\n",
    "df.rename(columns=column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_data = train_test_anomaly(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_threshold = [-0.03, +5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gtruth = test_data['anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyModelSelectionEnv(test_data, list_thresholds=list_threshold, list_gtruth=list_gtruth)\n",
    "environment = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(shape= () ,dtype=np.int32, minimum=0, maximum = 1, name='Select Models')\n",
    "observation_spec = tensor_spec.BoundedTensorSpec(shape=(1, 159), dtype=np.float64, minimum=-9999999, maximum=9999999, name='observation')\n",
    "time_step_spec = ts.time_step_spec(observation_spec)\n",
    "\n",
    "EPSILON = 0.05\n",
    "LAYERS = (50, 50, 50)\n",
    "LR = 0.005\n",
    "\n",
    "network = q_network.QNetwork(\n",
    "          input_tensor_spec=observation_spec,\n",
    "          action_spec=action_spec,\n",
    "          fc_layer_params=LAYERS\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps_agent = bandit_agents.neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(action_spec=action_spec, time_step_spec=time_step_spec, reward_network=network ,optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        epsilon=EPSILON,\n",
    "        emit_policy_info='predicted_rewards_mean',\n",
    "        info_fields_to_inherit_from_greedy=['predicted_rewards_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
    "  return trajectory.Trajectory(observation=initial_step.observation,\n",
    "                               action=action_step.action,\n",
    "                               policy_info=action_step.info,\n",
    "                               reward=final_step.reward,\n",
    "                               discount=final_step.discount,\n",
    "                               step_type=initial_step.step_type,\n",
    "                               next_step_type=final_step.step_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='60'\n",
       "                  max='60',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  60\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n",
      "tf.Tensor([1], shape=(1,), dtype=int32)\n",
      "*** Feature extraction started ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "              <p>\n",
       "                  Progress: 100% Complete\n",
       "              <p/>\n",
       "              <progress\n",
       "                  value='60'\n",
       "                  max='60',\n",
       "                  style='width: 25%',\n",
       "              >\n",
       "                  60\n",
       "              </progress>\n",
       "\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Feature extraction finished ***\n",
      "Trajectory(\n",
      "{'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>,\n",
      " 'observation': <tf.Tensor: shape=(1, 1, 159), dtype=float64, numpy=\n",
      "array([[[ 2.26980000e+04,  7.72500000e+00,  2.26980000e+04,\n",
      "          4.63224490e+04,  2.79830382e-01,  1.00000000e+01,\n",
      "          4.00000000e+01, -1.00000000e+00,  2.80000000e+01,\n",
      "          2.00000000e-02,  4.00000000e-02,  6.00000000e-02,\n",
      "          8.00000000e-02,  1.00000000e-01,  1.20000000e-01,\n",
      "          1.40000000e-01,  1.60000000e-01,  1.80000000e-01,\n",
      "          2.00000000e-01,  7.91601227e-01,  4.06120456e-03,\n",
      "          1.66111210e+00,  3.34111507e+00,  7.36808086e-02,\n",
      "          1.29475394e+00,  1.95294960e+00,  1.86123572e+00,\n",
      "          2.19119116e+00,  2.16909364e+00,  1.59364940e+00,\n",
      "          2.79607727e+00,  1.45212482e+01,  1.15778395e+01,\n",
      "          4.01796542e+00,  2.99860121e+00,  7.39479772e+00,\n",
      "          4.90333133e+00,  1.33782826e+01,  3.41726431e-02,\n",
      "          1.11810365e+00,  7.52237415e+00,  3.54358152e+00,\n",
      "          8.02744048e+00,  5.27966228e+00,  1.54745827e+00,\n",
      "          2.74104000e+00,  4.00000000e+00,  1.10000000e+01,\n",
      "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
      "          0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
      "          6.88187340e-01,  2.57500000e+01, -6.20707874e-01,\n",
      "          1.37894586e-01,  3.60311285e-01,  6.84939622e-01,\n",
      "          3.60311285e-01,  6.84939622e-01,  1.55143422e-01,\n",
      "          2.48451223e-01,  2.46198829e-01,  2.61687905e-01,\n",
      "          2.46198829e-01,  2.48451223e-01,  1.55143422e-01,\n",
      "         -4.89609461e+01,  3.53421622e+01,  3.61268484e+01,\n",
      "          7.12818466e+01, -3.55321307e+01, -2.61286202e+01,\n",
      "          5.06544728e+01,  5.26621976e+01,  5.18692969e+01,\n",
      "          1.32025689e+02, -1.32089714e+01,  9.94762524e+01,\n",
      "          5.20000000e+01,  5.27086699e-02,  4.80000000e+01,\n",
      "          1.54400000e+01,  1.23904000e+01,  1.69183673e+01,\n",
      "         -2.24489796e-01,  1.20000000e+01,  1.30000000e+01,\n",
      "          1.40000000e+01, -1.00000000e+00,  1.60000000e+01,\n",
      "         -1.00000000e+00,  1.60000000e+01,  1.00000000e+00,\n",
      "          5.30000000e+01,  1.80000000e+01,  4.40000000e+01,\n",
      "          2.13063371e+01,  8.34748969e+02,  6.21641937e-01,\n",
      "          5.21488595e-02,  1.96634196e+01, -1.09072278e+00,\n",
      "         -9.97993934e+03,  8.91123036e-01,  1.60369858e+00,\n",
      "          9.00000000e+00,  4.80000000e+01,  0.00000000e+00,\n",
      "          3.23194732e-01, -9.12235968e-04,  1.72556237e+01,\n",
      "          4.58134583e-01,  1.46821797e+01,  8.29000000e+02,\n",
      "          2.15566400e+02,  3.84708427e-01,  2.14529135e+00,\n",
      "          4.10962099e+00,  5.95088456e+00,  7.60396417e+00,\n",
      "          9.14722093e+00,  1.08745686e+01,  1.31990892e+01,\n",
      "          1.63880984e+01,  1.18971149e+01,  1.64722196e+01,\n",
      "          1.90229998e+01,  2.13785685e+01,  2.24634749e+01,\n",
      "          2.22234741e+01,  2.13384685e+01,  2.08067644e+01,\n",
      "          2.15474940e+01,  2.17851643e+00,  1.18908932e+01,\n",
      "          1.63319240e+01,  1.85737863e+01,  2.05336350e+01,\n",
      "          2.11373469e+01,  2.02536700e+01,  1.83595750e+01,\n",
      "          1.60843243e+01,  1.39901655e+01,  1.41393342e+02,\n",
      "          2.66731743e+02,  3.44985538e+02,  4.21630166e+02,\n",
      "          4.46787435e+02,  4.10211150e+02,  3.37073995e+02,\n",
      "          2.58705489e+02,  1.95724730e+02,  1.80000000e+01]]])>,\n",
      " 'action': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'policy_info': PolicyInfo(log_probability=(), predicted_rewards_mean=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-129.22243, 1063.7086 ]], dtype=float32)>, multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=()),\n",
      " 'next_step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>,\n",
      " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.398], dtype=float32)>,\n",
      " 'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rojan/Desktop/projects/MAB-Model-Selection/MAB-Based-Model-Selection/venv/lib/python3.9/site-packages/tf_agents/utils/common.py:1539: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return state is not None and state is not () and state is not []\n",
      "/home/rojan/Desktop/projects/MAB-Model-Selection/MAB-Based-Model-Selection/venv/lib/python3.9/site-packages/tf_agents/utils/common.py:1539: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  return state is not None and state is not () and state is not []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All of the Tensors in `value` must have two outer dimensions. Specifically, tensors must have shape `[B, T] + spec.shape`.\nFull shapes of value tensors:\n  Trajectory(\n{'step_type': TensorShape([1]),\n 'observation': TensorShape([1, 1, 159]),\n 'action': TensorShape([1]),\n 'policy_info': PolicyInfo(log_probability=(), predicted_rewards_mean=TensorShape([1, 2]), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=()),\n 'next_step_type': TensorShape([1]),\n 'reward': TensorShape([1]),\n 'discount': TensorShape([1])}).\nExpected shapes (excluding the two outer dimensions):\n  Trajectory(\n{'step_type': TensorShape([]),\n 'observation': TensorShape([1, 159]),\n 'action': TensorShape([]),\n 'policy_info': PolicyInfo(log_probability=(), predicted_rewards_mean=TensorShape([2]), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=()),\n 'next_step_type': TensorShape([]),\n 'reward': TensorShape([]),\n 'discount': TensorShape([])}).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m experience \u001b[38;5;241m=\u001b[39m trajectory_for_bandit(step, action_step, next_step)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(experience)\n\u001b[0;32m---> 10\u001b[0m \u001b[43meps_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m step \u001b[38;5;241m=\u001b[39m next_step\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m complete\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/MAB-Model-Selection/MAB-Based-Model-Selection/venv/lib/python3.9/site-packages/tf_agents/agents/tf_agent.py:348\u001b[0m, in \u001b[0;36mTFAgent.train\u001b[0;34m(self, experience, weights, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    343\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find _train_fn.  Did \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.__init__ call super?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m       \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    345\u001b[0m   )\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_functions:\n\u001b[0;32m--> 348\u001b[0m   loss_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexperience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m   loss_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(experience\u001b[38;5;241m=\u001b[39mexperience, weights\u001b[38;5;241m=\u001b[39mweights, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/projects/MAB-Model-Selection/MAB-Based-Model-Selection/venv/lib/python3.9/site-packages/tf_agents/utils/common.py:193\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m check_tf1_allowed()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_eager_been_enabled():\n\u001b[1;32m    191\u001b[0m   \u001b[38;5;66;03m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[1;32m    192\u001b[0m   \u001b[38;5;66;03m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m resource_variables_enabled():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[0;32m~/Desktop/projects/MAB-Model-Selection/MAB-Based-Model-Selection/venv/lib/python3.9/site-packages/tf_agents/bandits/agents/greedy_reward_prediction_agent.py:244\u001b[0m, in \u001b[0;36mGreedyRewardPredictionAgent._train\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, experience, weights):\n\u001b[0;32m--> 244\u001b[0m   experience \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    247\u001b[0m     loss_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(experience, weights\u001b[38;5;241m=\u001b[39mweights, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/projects/MAB-Model-Selection/MAB-Based-Model-Selection/venv/lib/python3.9/site-packages/tf_agents/agents/data_converter.py:365\u001b[0m, in \u001b[0;36mAsTrajectory.__call__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    364\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput type not supported: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(value))\n\u001b[0;32m--> 365\u001b[0m \u001b[43m_validate_trajectory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrajectory_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_outer_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outer_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m value \u001b[38;5;241m=\u001b[39m nest_utils\u001b[38;5;241m.\u001b[39mprune_extra_keys(\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_context\u001b[38;5;241m.\u001b[39mtrajectory_spec, value\n\u001b[1;32m    373\u001b[0m )\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/Desktop/projects/MAB-Model-Selection/MAB-Based-Model-Selection/venv/lib/python3.9/site-packages/tf_agents/agents/data_converter.py:197\u001b[0m, in \u001b[0;36m_validate_trajectory\u001b[0;34m(value, trajectory_spec, sequence_length, num_outer_dims)\u001b[0m\n\u001b[1;32m    193\u001b[0m   shape_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    194\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwo outer dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_outer_dims \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone outer dimension\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    195\u001b[0m   )\n\u001b[1;32m    196\u001b[0m   shape_prefix_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[B, T]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_outer_dims \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[B]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 197\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll of the Tensors in `value` must have \u001b[39m\u001b[38;5;132;01m{shape_str}\u001b[39;00m\u001b[38;5;124m. Specifically, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    199\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensors must have shape `\u001b[39m\u001b[38;5;132;01m{shape_prefix_str}\u001b[39;00m\u001b[38;5;124m + spec.shape`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    200\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFull shapes of value tensors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{debug_str_1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    201\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected shapes (excluding the \u001b[39m\u001b[38;5;132;01m{shape_str}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{debug_str_2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    202\u001b[0m           shape_str\u001b[38;5;241m=\u001b[39mshape_str,\n\u001b[1;32m    203\u001b[0m           debug_str_1\u001b[38;5;241m=\u001b[39mdebug_str_1,\n\u001b[1;32m    204\u001b[0m           debug_str_2\u001b[38;5;241m=\u001b[39mdebug_str_2,\n\u001b[1;32m    205\u001b[0m           shape_prefix_str\u001b[38;5;241m=\u001b[39mshape_prefix_str,\n\u001b[1;32m    206\u001b[0m       )\n\u001b[1;32m    207\u001b[0m   )\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# If we have a time dimension and a train_sequence_length, make sure they\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# match.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sequence_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: All of the Tensors in `value` must have two outer dimensions. Specifically, tensors must have shape `[B, T] + spec.shape`.\nFull shapes of value tensors:\n  Trajectory(\n{'step_type': TensorShape([1]),\n 'observation': TensorShape([1, 1, 159]),\n 'action': TensorShape([1]),\n 'policy_info': PolicyInfo(log_probability=(), predicted_rewards_mean=TensorShape([1, 2]), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=()),\n 'next_step_type': TensorShape([1]),\n 'reward': TensorShape([1]),\n 'discount': TensorShape([1])}).\nExpected shapes (excluding the two outer dimensions):\n  Trajectory(\n{'step_type': TensorShape([]),\n 'observation': TensorShape([1, 159]),\n 'action': TensorShape([]),\n 'policy_info': PolicyInfo(log_probability=(), predicted_rewards_mean=TensorShape([2]), multiobjective_scalarized_predicted_rewards_mean=(), predicted_rewards_optimistic=(), predicted_rewards_sampled=(), bandit_policy_type=()),\n 'next_step_type': TensorShape([]),\n 'reward': TensorShape([]),\n 'discount': TensorShape([])})."
     ]
    }
   ],
   "source": [
    "step = environment.reset()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    action_step = eps_agent.collect_policy.action(step)\n",
    "    print(action_step.action)\n",
    "    next_step = environment.step(action_step.action)\n",
    "    experience = trajectory_for_bandit(step, action_step, next_step) # Error in this line. Figure out the trajectory for bandit data type. \n",
    "    print(experience)\n",
    "    eps_agent.train(experience=experience)\n",
    "    step = next_step\n",
    "    print(f'step {i} complete')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(shape= () ,dtype=np.int32, minimum=0, maximum = 1, name='Select Models')\n",
    "observation_spec = tensor_spec.BoundedTensorSpec(shape=(1, 318), dtype=np.float64, minimum=-9999999, maximum=9999999, name='observation')\n",
    "time_step_spec = ts.time_step_spec(observation_spec)\n",
    "\n",
    "linucb_agent = bandit_agents.lin_ucb_agent.LinearUCBAgent(action_spec=action_spec, time_step_spec=time_step_spec, tikhonov_weight=0.001, alpha=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = environment.reset()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    action_step = agent._collect_policy.action(step)\n",
    "    next_step = environment.step(action_step)\n",
    "    experience = trajectory_for_bandit(step, action_step, next_step)\n",
    "    print(experience)\n",
    "    agent.train(experience=experience)\n",
    "    step = next_step\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
