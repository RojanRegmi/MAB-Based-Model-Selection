{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Environment.environment import MyModelSelectionEnv\n",
    "from src.utils import train_test_anomaly\n",
    "from src.Environment import trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_agents.bandits.agents as bandit_agents\n",
    "from tf_agents.specs import array_spec, tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.agents import data_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.bandits.agents import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../datasets/Dodgers/101-freeway-traffic.test.out'\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = {'-1': 'value', '1': 'anomaly'}\n",
    "df.rename(columns=column_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_data = train_test_anomaly(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_threshold = [-0.03, +5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gtruth = test_data['anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyModelSelectionEnv(test_data, list_thresholds=list_threshold, list_gtruth=list_gtruth)\n",
    "environment = tf_py_environment.TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_list = []\n",
    "\n",
    "for _ in range(10):\n",
    "\n",
    "    action = np.random.randint(0, 1)\n",
    "\n",
    "    aa = env._step(action)\n",
    "    print(env.pointer)\n",
    "    op_list.append(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in op_list:\n",
    "    print(i.reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = env._reset()\n",
    "aa.observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = environment.action_spec()\n",
    "observation_spec = environment.time_step_spec().observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = env._step(0)\n",
    "bb.reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = environment.action_spec()\n",
    "observation_spec = environment.time_step_spec().observation\n",
    "\n",
    "\n",
    "EPSILON = 0.1\n",
    "LAYERS = (50, 50, 50)\n",
    "LR = 0.005\n",
    "\n",
    "network = q_network.QNetwork(\n",
    "          input_tensor_spec=observation_spec,\n",
    "          action_spec=action_spec,\n",
    "          fc_layer_params=LAYERS\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps_agent = bandit_agents.neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(action_spec=action_spec, time_step_spec=environment.time_step_spec(), reward_network=network ,optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        epsilon=EPSILON,\n",
    "        emit_policy_info='predicted_rewards_mean',\n",
    "        info_fields_to_inherit_from_greedy=['predicted_rewards_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = environment.reset()\n",
    "\n",
    "action_step = eps_agent.collect_policy.action(step)\n",
    "\n",
    "next_step = environment.step(action_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_agent.policy.trajectory_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_step.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = tensor_spec.BoundedTensorSpec(shape= () ,dtype=np.int32, minimum=0, maximum = 1, name='Select Models')\n",
    "observation_spec = tensor_spec.BoundedTensorSpec(shape=(1, 318), dtype=np.float64, minimum=-9999999, maximum=9999999, name='observation')\n",
    "time_step_spec = ts.time_step_spec(observation_spec)\n",
    "\n",
    "linucb_agent = bandit_agents.lin_ucb_agent.LinearUCBAgent(action_spec=action_spec, time_step_spec=time_step_spec, tikhonov_weight=0.001, alpha=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = environment.reset()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    action_step = agent._collect_policy.action(step)\n",
    "    next_step = environment.step(action_step)\n",
    "    experience = trajectory_for_bandit(step, action_step, next_step)\n",
    "    print(experience)\n",
    "    agent.train(experience=experience)\n",
    "    step = next_step\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def trajectory_for_bandit(initial_step, action_step, final_step):\n",
    "  return trajectory.Trajectory(observation=initial_step.observation,\n",
    "                               action=action_step.action,\n",
    "                               policy_info=action_step.info,\n",
    "                               reward=final_step.reward,\n",
    "                               discount=final_step.discount,\n",
    "                               step_type=initial_step.step_type,\n",
    "                               next_step_type=final_step.step_type)\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "  All shapes are = (1, 1, X)\n",
    "\n",
    "  def trajectory_for_bandit(initial_step, action_step, final_step):\n",
    "\n",
    "  observation = initial_step.observation\n",
    "  action = tf.expand_dims(tf.expand_dims(action_step.action, 0), 0)\n",
    "  #policy_info = tf.expand_dims(action_step.info, 0)\n",
    "  policy_info = tf.expand_dims(action_step.info.predicted_rewards_mean, 0)\n",
    "  reward = tf.expand_dims(tf.expand_dims(final_step.reward, 0), 0)\n",
    "  discount = tf.expand_dims(tf.expand_dims(final_step.discount, 0), 0)\n",
    "  step_type = tf.expand_dims(tf.expand_dims(initial_step.step_type, 0), 0)\n",
    "  next_step_type = tf.expand_dims(tf.expand_dims(final_step.step_type, 0), 0)\n",
    "\n",
    "  return trajectory.Trajectory(\n",
    "        observation=observation,\n",
    "        action=action,\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=discount,\n",
    "        step_type=step_type,\n",
    "        next_step_type=next_step_type\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"def trajectory_for_bandit(initial_step, action_step, final_step):\n",
    "\n",
    "  observation = tf.squeeze(initial_step.observation, axis=1)\n",
    "  action = tf.squeeze(action_step.action),\n",
    "  #policy_info = tf.expand_dims(action_step.info, 0)\n",
    "  policy_info = action_step.info.predicted_rewards_mean\n",
    "  reward = final_step.reward,\n",
    "  discount = final_step.discount,\n",
    "  step_type = initial_step.step_type,\n",
    "  next_step_type = final_step.step_type\n",
    "\n",
    "  return trajectory.Trajectory(\n",
    "        observation=observation,\n",
    "        action=action,\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=discount,\n",
    "        step_type=step_type,\n",
    "        next_step_type=next_step_type\n",
    "    )\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
    "\n",
    "  observation = initial_step.observation\n",
    "  action = tf.expand_dims(tf.expand_dimsc(action_step.action, 0), 0)\n",
    "  #policy_info = tf.expand_dims(action_step.info, 0)\n",
    "  policy_info = tf.expand_dims(action_step.info.predicted_rewards_mean, 0)\n",
    "  reward = tf.expand_dims(tf.expand_dims(final_step.reward, 0), 0)\n",
    "  discount = tf.expand_dims(tf.expand_dims(final_step.discount, 0), 0)\n",
    "  step_type = tf.expand_dims(tf.expand_dims(initial_step.step_type, 0), 0)\n",
    "  next_step_type = tf.expand_dims(tf.expand_dims(final_step.step_type, 0), 0)\n",
    "\n",
    "  return trajectory.Trajectory(\n",
    "        observation=observation,\n",
    "        action=action,\n",
    "        policy_info=policy_info,\n",
    "        reward=reward,\n",
    "        discount=discount,\n",
    "        step_type=step_type,\n",
    "        next_step_type=next_step_type\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory_for_bandit(initial_step, action_step, final_step):\n",
    "\n",
    "  observation = tf.cast(tf.squeeze(initial_step.observation, axis=1), dtype=tf.float64)\n",
    "  action = tf.squeeze(action_step.action),\n",
    "  #policy_info = tf.expand_dims(action_step.info, 0)\n",
    "  policy_info = action_step.info,\n",
    "  reward = tf.squeeze(final_step.reward),\n",
    "  discount = tf.squeeze(final_step.discount),\n",
    "  step_type = tf.squeeze(initial_step.step_type),\n",
    "  next_step_type = tf.squeeze(final_step.step_type)\n",
    "\n",
    "  return trajectory.Trajectory(\n",
    "        step_type=step_type,\n",
    "        observation=observation,\n",
    "        action=action,\n",
    "        policy_info=policy_info,\n",
    "        next_step_type=next_step_type,\n",
    "        reward=reward,\n",
    "        discount=discount\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = environment.reset()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    action_step = eps_agent.collect_policy.action(step)\n",
    "    print(action_step.action)\n",
    "    next_step = environment.step(action_step.action)\n",
    "    experience = trajectory_for_bandit(step, action_step, next_step) \n",
    "    print(experience)\n",
    "    eps_agent.train(experience)\n",
    "    step = next_step\n",
    "    print(f'step {i} complete')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp)\n",
    "print(eps_agent.training_data_spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
