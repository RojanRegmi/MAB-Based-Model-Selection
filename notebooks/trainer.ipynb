{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.Environment.environment import MyModelSelectionEnv\n",
    "from src.utils import train_test_anomaly\n",
    "from src.Components.data_processing import data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_agents.bandits.agents as bandit_agents\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network, NestFlatten\n",
    "from tf_agents.bandits.replay_buffers import bandit_replay_buffer\n",
    "from tf_agents.metrics import export_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data and Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../datasets/Dodgers/101-freeway-traffic.test.out'\n",
    "\n",
    "columns = ['value', 'anomaly']\n",
    "\n",
    "df = pd.read_csv(file_path, names=columns, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_data = train_test_anomaly(df)\n",
    "\n",
    "list_threshold = [-0.03, +5]\n",
    "list_gtruth = test_data['anomaly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_np = test_data['value'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyModelSelectionEnv(test_data, list_thresholds=list_threshold, list_gtruth=list_gtruth)\n",
    "environment = tf_py_environment.TFPyEnvironment(env) # Converts the PyEnvironment to TFEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Neural Epsilon Greedy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_spec = environment.action_spec()\n",
    "observation_spec = environment.time_step_spec().observation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EPSILON = 0.1\n",
    "LAYERS = (50, 50, 50)\n",
    "LR = 0.005\n",
    "\n",
    "TRAINING_LOOPS = 500\n",
    "steps_per_loop = 1\n",
    "async_steps_per_loop = 1\n",
    "\n",
    "network = q_network.QNetwork(\n",
    "          input_tensor_spec=observation_spec,\n",
    "          action_spec=action_spec,\n",
    "          fc_layer_params=LAYERS,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps_agent = bandit_agents.neural_epsilon_greedy_agent.NeuralEpsilonGreedyAgent(action_spec=environment.action_spec(), time_step_spec=environment.time_step_spec(), reward_network=network, optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=LR),\n",
    "        epsilon=EPSILON,\n",
    "        emit_policy_info='predicted_rewards_mean',\n",
    "        info_fields_to_inherit_from_greedy=['predicted_rewards_mean'])\n",
    "\n",
    "eps_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffers and Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec = eps_agent.policy.trajectory_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replay_buffer(\n",
    "    data_spec, batch_size, steps_per_loop, async_steps_per_loop\n",
    "):\n",
    "  \"\"\"Return a `TFUniformReplayBuffer` for the given `agent`.\"\"\"\n",
    "  return bandit_replay_buffer.BanditReplayBuffer(\n",
    "      data_spec=data_spec,\n",
    "      batch_size=batch_size,\n",
    "      max_length=steps_per_loop * async_steps_per_loop,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = get_replay_buffer(\n",
    "      data_spec, environment.batch_size, steps_per_loop, async_steps_per_loop\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observers\n",
    "\n",
    "step_metric = tf_metrics.EnvironmentSteps()\n",
    "metrics = [\n",
    "      tf_metrics.NumberOfEpisodes(), # Counts the number of episodes in the environment\n",
    "      tf_metrics.AverageEpisodeLengthMetric(batch_size=environment.batch_size),   # Metric to compute the average episode length\n",
    "      tf_metrics.AverageReturnMetric(batch_size=environment.batch_size) # Metric to compute the average return\n",
    "  ]\n",
    "\n",
    "add_batch_fn = replay_buffer.add_batch # Adds a batch of items on the replay buffer\n",
    "\n",
    "observers = [add_batch_fn, step_metric] + metrics # List of observers for the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = dynamic_step_driver.DynamicStepDriver(\n",
    "      env=environment,\n",
    "      policy=eps_agent.collect_policy,\n",
    "      num_steps=steps_per_loop * environment.batch_size,\n",
    "      observers=observers,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer Values\n",
    "\n",
    "\"\"\"\n",
    "    The .as_dataset method creates and returns a dataset as entries from the buffer.\n",
    "\n",
    "    A single entry from the dataset is the result of the following pipeline:\n",
    "\n",
    "    - Sample sequences from the underlying data store\n",
    "    - (Optional) Process them with 'sequence_preprocess_fn'\n",
    "    - (Optional) Split them into subsequences of length num_steps\n",
    "    - (Optional) Batch them into batches of size 'sample_batch_size'\n",
    "\n",
    "    In practice, this pipeline is executed in parallel as much as possible if num_parallel_calls != 1.\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "dataset_it = iter(\n",
    "        replay_buffer.as_dataset(\n",
    "            sample_batch_size=1,\n",
    "            num_steps=1,\n",
    "            single_deterministic_pass=True,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_step, metrics):\n",
    " \n",
    "    driver.run()\n",
    "     \n",
    "    batch_size = driver.env.batch_size\n",
    "    dataset_it = iter(\n",
    "        replay_buffer.as_dataset(\n",
    "            sample_batch_size=batch_size,\n",
    "            num_steps=1,\n",
    "            single_deterministic_pass=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    meter = driver.observers[1:]\n",
    "    for batch_id in range(async_steps_per_loop):\n",
    "      experience, unused_buffer_info = dataset_it.get_next()\n",
    "      loss_info = eps_agent.train(experience)\n",
    "\n",
    "      export_utils.export_metrics(\n",
    "          step=train_step * async_steps_per_loop + batch_id,\n",
    "          metrics=meter,\n",
    "          loss_info=loss_info,\n",
    "      )\n",
    "\n",
    "    replay_buffer.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 100):\n",
    "\n",
    "    training_loop(train_step=i, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = eps_agent.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained Policy Steps in Environment\n",
    "\n",
    "act = 0\n",
    "policy_state = policy.get_initial_state(batch_size=1)\n",
    "act_list = []\n",
    "score_list = []\n",
    "\n",
    "for _ in range(50):\n",
    "\n",
    "    time_step = environment.step(act)\n",
    "    policy_return = policy.action(time_step, policy_state)\n",
    "    act = policy_return.action.numpy()\n",
    "    _, scr = env._apply_action(act)\n",
    "    score_list.append(scr)\n",
    "\n",
    "    \n",
    "    policy_state = policy_return.state\n",
    "    act_list.append(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
